{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env = e.unwrapped\n",
    "\n",
    "env.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Possible Actions:  2\n"
     ]
    }
   ],
   "source": [
    "state_size = 4\n",
    "action_size = env.action_space.n\n",
    "print(\"Possible Actions: \", action_size)\n",
    "\n",
    "max_episodes = 300\n",
    "learning_rate = 0.01\n",
    "gamma = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalized_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cum = 0.\n",
    "    \n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cum = cum*gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cum\n",
    "        \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / std\n",
    "    \n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"inputs\"):\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, state_size], name = 'input')\n",
    "    actions = tf.placeholder(tf.int32, [None, action_size], name = 'actions')\n",
    "    discounted_episode_rewards_ = tf.placeholder(tf.float32, [None,], name = 'discounted_episode_rewards')\n",
    "    \n",
    "    mean_reward_ = tf.placeholder(tf.float32, name = 'mean_reward')\n",
    "    \n",
    "    with tf.name_scope('fc1'):\n",
    "        fc1 = tf.contrib.layers.fully_connected(inputs = inputs_,\n",
    "                                               num_outputs = 10,\n",
    "                                               activation_fn = tf.nn.relu,\n",
    "                                               weights_initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "    with tf.name_scope('fc2'):\n",
    "        fc2 = tf.contrib.layers.fully_connected(inputs = fc1,\n",
    "                                               num_outputs = action_size,\n",
    "                                               activation_fn = tf.nn.relu,\n",
    "                                               weights_initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "    with tf.name_scope('fc3'):\n",
    "        fc3 = tf.contrib.layers.fully_connected(inputs = fc2,\n",
    "                                               num_outputs = action_size,\n",
    "                                               activation_fn = None,\n",
    "                                               weights_initializer = tf.contrib.layers.xavier_initializer())\n",
    "        \n",
    "    with tf.name_scope('softmax'):\n",
    "        action_distribution = tf.nn.softmax(fc3)\n",
    "        \n",
    "    with tf.name_scope('loss'):\n",
    "        neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(logits = fc3,\n",
    "                                                                 labels = actions)\n",
    "        loss = tf.reduce_mean(neg_log_prob*discounted_episode_rewards_)\n",
    "    \n",
    "    with tf.name_scope('train'):\n",
    "        train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter('/tensorboard/pg/1')\n",
    "\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "tf.summary.scalar('reward_mean', mean_reward_)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "episode = 0\n",
    "episode_states, episode_actions, episode_rewards = [], [], []\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for episode in range(1, max_episodes+1):\n",
    "        episode_rewards_sum = 0\n",
    "        \n",
    "        state = env.reset()\n",
    "        env.render()\n",
    "        \n",
    "        while True:\n",
    "            action_probability_distribution = sess.run(action_distribution,\n",
    "                                                      feed_dict = {inputs_: state.reshape([1, 4])})\n",
    "            \n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p = action_probability_distribution.ravel())\n",
    "            \n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            episode_states.append(state)\n",
    "            \n",
    "            action_ = np.zeros(action_size)\n",
    "            action_[action] = 1\n",
    "            episode_actions.append(action_)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "            \n",
    "            if done:\n",
    "                episode_rewards_sum = np.sum(episode_rewards)\n",
    "                all_rewards.append(episode_rewards_sum)\n",
    "                total_rewards = np.sum(all_rewards)\n",
    "                mean_reward = np.divide(total_rewards, episode)\n",
    "                \n",
    "                maximumRewardRecorded = np.amax(all_rewards)\n",
    "                \n",
    "                print(\"======================================\")\n",
    "                print(\"Episode: \", episode)\n",
    "                print(\"Reward: \", episode_rewards_sum)\n",
    "                print(\"Mean reward: \", mean_reward)\n",
    "                print(\"Max reward so far: \", maximumRewardRecorded)\n",
    "                \n",
    "                discounted_episode_rewards = discount_and_normalized_rewards(episode_rewards)\n",
    "                \n",
    "                loss_, _ = sess.run([loss, train_opt], feed_dict = {inputs_: np.vstack(np.array(episode_states)),\n",
    "                                                                  actions: np.vstack(np.array(episode_actions)),\n",
    "                                                                  discounted_episode_rewards_: discounted_episode_rewards})\n",
    "                \n",
    "                summary = sess.run(write_op, feed_dict = {inputs_: np.vstack(np.array(episode_states)),\n",
    "                                                          actions: np.vstack(np.array(episode_actions)),\n",
    "                                                          discounted_episode_rewards_: discounted_episode_rewards,\n",
    "                                                          mean_reward_: mean_reward})\n",
    "                \n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "                episode_states, episode_actions, episode_rewards = [], [], []\n",
    "                \n",
    "                break;\n",
    "                \n",
    "            state = new_state\n",
    "            \n",
    "        if episode%10 == 0:\n",
    "            saver.save(sess, './models/model.ckpt')\n",
    "            print(\"Model Saved\")\n",
    "\n",
    "env.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing - Agent plays the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_episodes = 10\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    env.reset()\n",
    "    rewards = []\n",
    "    \n",
    "    saver.restore(sess, './models/model.ckpt')\n",
    "    \n",
    "    for episode in range(1, test_episodes+1):\n",
    "        state = env.reset()\n",
    "        env.render()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "        print(\"===============================\")\n",
    "        print(\"Episode: \", episode)\n",
    "        \n",
    "        while True:\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={inputs_: state.reshape([1,state_size])})\n",
    "            \n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            total_rewards += reward\n",
    "            \n",
    "            if done:\n",
    "                rewards.append(total_rewards)\n",
    "                print (\"Score: \", total_rewards)\n",
    "                break\n",
    "            \n",
    "            state = new_state\n",
    "            \n",
    "    env.close()\n",
    "    print (\"Average Score: \" +  str(sum(rewards)/test_episodes))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
