{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import retro\n",
    "\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import random\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m retro.import .  # Import game ROM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = retro.make(game = 'SpaceInvaders-Atari2600') # Load the game from ROM\n",
    "\n",
    "print(\"The size of the frame is: \", env.observation_space)  # Size of a frame of game window\n",
    "print(\"The action size is: \", env.action_space.n)  # Number of actions possible in the game\n",
    "\n",
    "# One hot encoded vectors of all actions\n",
    "possible_actions = np.array(np.identity(env.action_space.n, dtype=int).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the game frame\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    gray_frame = rgb2gray(frame)  # Color does not add any additional information so its computationally efficient to\n",
    "                                  # convert to grayscale\n",
    "    \n",
    "    cropped_frame = gray_frame[0:-12, 4:-12] # Cropping unecessary area from the frame\n",
    "    \n",
    "    normalized_frame = cropped_frame / 255.\n",
    "    \n",
    "    preprocessed_frame = transform.resize(normalized_frame, [110, 84])\n",
    "    \n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4  # Stacking 4 frames to give a picture of motion to the model\n",
    "\n",
    "stacked_frame = deque([np.zeros((110, 84), dtype=np.int) for i in range(stack_size)], maxlen = 4)\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    # for first frame, stack it four times\n",
    "    if is_new_episode: \n",
    "        stacked_frame = deque([np.zeros((110, 84), dtype=np.int) for i in range(stack_size)], maxlen = 4)\n",
    "        \n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        stacked_state = np.stack(stacked_frames, axis = 2)\n",
    "        \n",
    "    # if not the first frame, enqueue the latest frame and dequeue the oldest frame\n",
    "    else:  \n",
    "        stacked_frames.append(frame)\n",
    "        \n",
    "        stacked_state = np.stack(stacked_frames, axis = 2)\n",
    "        \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = [110, 84, 4]  # Size of input to the model\n",
    "\n",
    "action_size = env.action_space.n\n",
    "\n",
    "total_episodes = 25  # Total number of training games\n",
    "max_steps = 10000    # Maximum steps to be taken in a game\n",
    "batch_size = 64      # Batch size input to the model\n",
    "\n",
    "learning_rate = 0.00025\n",
    "epsilon_start = 1.   # max exploration rate\n",
    "epsilon_end = 0.01   # min exploration rate\n",
    "decay_rate = 0.00001 # decay rate per game\n",
    "\n",
    "gamma = 0.9          #discount factor\n",
    "\n",
    "pretrain_length = batch_size  # Initial memory size\n",
    "memory_size = 500000  # Maximum memory size\n",
    "\n",
    "training = True    # Boolen value to train or not\n",
    "episode_render = True # Boolean value to see the agent train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked_frame [110, 84, 4] ---> conv1 ----> [26, 20, 32] (elu) ----> conv2 ----> [12, 9, 64] (elu)\n",
    "# -----> conv3 -----> [5, 4, 64] (elu) ----> Flatten -----> [1280,] -----> Dense ----> [32,] (elu)\n",
    "# ------> Dense ----> [self.action,]\n",
    "\n",
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name = 'DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            # placeholders for inputs and actions\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name = 'inputs')\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, self.action_size], name = 'actions')\n",
    "            \n",
    "            # Placeholder of target variable\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name = 'target')\n",
    "            \n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8, 8],\n",
    "                                         strides = [4, 4],\n",
    "                                         padding = 'valid',\n",
    "                                         kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = 'conv1')\n",
    "            \n",
    "            self.conv1_out = tf.nn.elu(self.conv1, name = 'conv1_out')\n",
    "            \n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                         filters = 64,\n",
    "                                         kernel_size = [4, 4],\n",
    "                                         strides = [2, 2],\n",
    "                                         padding = 'valid',\n",
    "                                         kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = 'conv2')\n",
    "            \n",
    "            self.conv2_out = tf.nn.elu(self.conv2, name = 'conv2_out')\n",
    "            \n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                         filters = 64,\n",
    "                                         kernel_size = [3, 3],\n",
    "                                         strides = [2, 2],\n",
    "                                         padding = 'valid',\n",
    "                                         kernel_initializer = tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = 'conv3')\n",
    "            \n",
    "            self.conv3_out = tf.nn.elu(self.conv3, name = 'conv3_out')\n",
    "            \n",
    "            self.flatten = tf.contrib.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                     units = 32,\n",
    "                                     activation = tf.nn.elu,\n",
    "                                     kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                     name = 'fc1')\n",
    "            \n",
    "            self.output = tf.layers.dense(inputs = self.fc,\n",
    "                                         kernel_initializer = tf.contrib.layers.xavier_initializer(),\n",
    "                                         units = self.action_size,\n",
    "                                         activation = None)\n",
    "            \n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_)) # Q-value of the taken action\n",
    "            \n",
    "            # Mean squared loss function\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            # Adam optimizer\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)           \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Initialize network\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memory class to store experience (stacked frames, action, reward, next state, boolean to know game finished or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "        \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "    \n",
    "    # return random batch of experiences\n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        index = np.random.choice(np.arange(buffer_size),\n",
    "                                size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially filling memory with random experiences from the start frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    if i==0:\n",
    "        state = env.reset()\n",
    "        \n",
    "        state, stacked_frames = stack_frames(stacked_frame,\n",
    "                                            state, True)\n",
    "        \n",
    "    choice = random.randint(1, len(possible_actions)) - 1\n",
    "    action = possible_actions[choice]\n",
    "    next_state, reward, done, _ = env.step(action)\n",
    "    \n",
    "#     env.render()\n",
    "    \n",
    "    next_state, stacked_frames = stack_frames(stacked_frames,\n",
    "                                             next_state, False)\n",
    "    \n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)  # all zeros mark the final state\n",
    "        \n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        state = env.reset()\n",
    "        \n",
    "        state, stacked_frames = stack_frames(stacked_frame,\n",
    "                                            state, True)\n",
    "        \n",
    "    else:\n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "\n",
    "tf.summary.scalar(\"Loss\", DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(explore_start, explore_stop, decay_rate, decay_step, state, actions):\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    \n",
    "    explore_probability = epsilon_end + (epsilon_start - epsilon_end) * np.exp(-decay_rate * decay_step)\n",
    "\n",
    "    if explore_probability > exp_exp_tradeoff:\n",
    "        choice = random.randint(1, len(possible_actions)) - 1\n",
    "        action = possible_actions[choice]\n",
    "        \n",
    "    else:\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_ : state.reshape((1, *state.shape))})\n",
    "        \n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[choice]\n",
    "        \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "rewards_list = []\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "        #Training from scratch: comment the line below and uncomment the line next to it\n",
    "        saver.restore(sess, \"./models/model.ckpt\")\n",
    "        \n",
    "        \n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        decay_step = 0  \n",
    "        \n",
    "        for episode in range(1, total_episodes+1):\n",
    "            step = 0\n",
    "            \n",
    "            episode_rewards = []\n",
    "            \n",
    "            state = env.reset()\n",
    "            \n",
    "            state, stacked_frames = stack_frames(stacked_frame, state, True)\n",
    "            \n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                \n",
    "                decay_step += 1 # decaying exploration for each step in the game\n",
    "                \n",
    "                # Predict the action output by the model\n",
    "                action, explore_probability = predict_action(epsilon_start,\n",
    "                                                            epsilon_end,\n",
    "                                                            decay_rate,\n",
    "                                                            decay_step,\n",
    "                                                            state,\n",
    "                                                            possible_actions)\n",
    "                \n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                \n",
    "                if episode_render:\n",
    "                    env.render()\n",
    "                \n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                if done:\n",
    "                    next_state = np.zeros((110, 84), dtype = np.int)\n",
    "                    \n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames,\n",
    "                                                             next_state, False)\n",
    "                    \n",
    "                    step = max_steps # ending the current episode\n",
    "                    \n",
    "                    total_reward = np.sum(episode_rewards)\n",
    "                    \n",
    "                    print('Episode: {}'.format(episode),\n",
    "                         'Total reward: {}'.format(total_reward),\n",
    "                         'Explore P: {:.4f}'.format(explore_probability),\n",
    "                         'Training Loss {:.4f}'.format(loss))\n",
    "                    \n",
    "                    rewards_list.append(total_reward)\n",
    "                    \n",
    "                    memory.add((state, action, reward, next_state, done)) # add experience to memory\n",
    "                    \n",
    "                else:\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames,\n",
    "                                                             next_state, False)\n",
    "                    \n",
    "                    memory.add((state, action, reward, next_state, done))  # add experience to memory\n",
    "                    \n",
    "                # sample a batch from memory\n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin = 3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch])\n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin = 3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "                \n",
    "                target_Qs_batch = []\n",
    "                \n",
    "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                for i in range(len(batch)):\n",
    "                    terminal = dones_mb[i]\n",
    "                    \n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma*np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "                        \n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "                                  feed_dict = {DQNetwork.inputs_: states_mb,\n",
    "                                              DQNetwork.target_Q: targets_mb,\n",
    "                                              DQNetwork.actions_: actions_mb})\n",
    "                \n",
    "                summary = sess.run(write_op, feed_dict = {DQNetwork.inputs_: states_mb,\n",
    "                                                         DQNetwork.target_Q: targets_mb,\n",
    "                                                         DQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "            print(\"Training game \",episode, \" ends\")\n",
    "            if episode%2 == 0:\n",
    "                save_path = saver.save(sess, './models/model.ckpt')\n",
    "                print(\"Model Saved\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing - Agent plays the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_episodes = 1\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./models/model.ckpt\") # Load the trained weights\n",
    "    \n",
    "    total_test_rewards = []\n",
    "    \n",
    "    for episode in range(test_episodes):\n",
    "        total_rewards = 0\n",
    "        \n",
    "        state = env.reset()\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "        while True:\n",
    "            state = state.reshape((1, *state_size))\n",
    "                                    \n",
    "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state})\n",
    "            \n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[choice]\n",
    "            print(action)\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            env.render()\n",
    "            \n",
    "            total_rewards += reward\n",
    "            \n",
    "            if done:\n",
    "                print(\"Score: \", total_rewards)\n",
    "                total_test_rewards.append(total_rewards)\n",
    "                break\n",
    "                \n",
    "            next_state, stacked_frames = stack_frames(stacked_frames, next_state, False)\n",
    "            state = next_state\n",
    "            \n",
    "        env.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
