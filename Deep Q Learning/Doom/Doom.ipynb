{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from vizdoom import *\n",
    "\n",
    "import random\n",
    "import time\n",
    "from skimage import transform\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a game environment and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The game has three possible actions\n",
    "\n",
    "1. Move left\n",
    "2. Move right\n",
    "3. Shoot\n",
    "\n",
    "The agent is rewarded points for each action and state\n",
    "\n",
    "1. For shooting the monster - +101 points\n",
    "2. For missing - -5 points\n",
    "3. For being alive - -1 point\n",
    "\n",
    "The reward system will force the agent to kill the monster as soon as possible and without wasting ammos.\n",
    "The reward system is preloaded in the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_environment():\n",
    "    game = DoomGame()\n",
    "    \n",
    "    game.load_config(\"basic.cfg\")  # You may load different config for a different game scenario\n",
    "    \n",
    "    game.set_doom_scenario_path(\"basic.wad\") # You may load different wad for a different game scenario\n",
    "    \n",
    "    game.init()\n",
    "    \n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    \n",
    "    possible_actions = [left, right, shoot]\n",
    "    \n",
    "    return game, possible_actions\n",
    "\n",
    "def test_environment():\n",
    "    game = DoomGame()\n",
    "    game.load_config(\"basic.cfg\")\n",
    "    game.set_doom_scenario_path(\"basic.wad\")\n",
    "    game.init()\n",
    "    \n",
    "    left = [1, 0, 0]\n",
    "    right = [0, 1, 0]\n",
    "    shoot = [0, 0, 1]\n",
    "    \n",
    "    actions = [left, right, shoot]\n",
    "    \n",
    "    episodes = 10\n",
    "    \n",
    "    for i in range(episodes):\n",
    "        game.new_episode()\n",
    "        \n",
    "        while not game.is_episode_finished():\n",
    "            state = game.get_state() \n",
    "            img = state.screen_buffer # Returns the frame from a game (RGB Frame)\n",
    "            misc = state.game_variables\n",
    "            action = random.choice(actions)\n",
    "            print(action)\n",
    "            reward = game.make_action(action)\n",
    "            print(\"reward: \",reward)\n",
    "            time.sleep(0.02)\n",
    "            \n",
    "        print(\"Result: \",game.get_total_reward())\n",
    "        time.sleep(2)\n",
    "    \n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to make a test environment and check if game works fine\n",
    "\n",
    "# test_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "game, possible_actions = create_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the game frame\n",
    "\n",
    "def preprocess_frame(frame):\n",
    "    frame = np.moveaxis(frame, 0, -1) # Re-order to have frame in (Height, Width, Channels) order\n",
    "    frame = rgb2gray(frame)     # Color does not add any additional information so its computationally efficient to\n",
    "                                  # convert to grayscale\n",
    "    \n",
    "    cropped_frame = frame[30: -10, 30: -30]  # Cropping unecessary area from the frame\n",
    "    \n",
    "    normalized_frame = cropped_frame/ 255.\n",
    "    \n",
    "    preprocessed_frame = transform.resize(normalized_frame, [84, 84])\n",
    "    \n",
    "    return preprocessed_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_size = 4   # Stacking 4 frames to give a picture of motion to the model\n",
    "\n",
    "stacked_frames = deque([np.zeros((84, 84), dtype = np.int) for i in range(stack_size)], maxlen = stack_size)\n",
    "\n",
    "def stack_frames(stacked_frames, state, is_new_episode):\n",
    "    frame = preprocess_frame(state)\n",
    "    \n",
    "    # for first frame, stack it four times\n",
    "    if is_new_episode:\n",
    "        stacked_frames = deque([np.zeros((84, 84), dtype = np.int) for i in range(stack_size)], maxlen = stack_size)\n",
    "\n",
    "        for i in range(stack_size):\n",
    "            stacked_frames.append(frame)\n",
    "            \n",
    "        stacked_state = np.stack(stacked_frames, axis = 2)\n",
    "        \n",
    "    # if not the first frame, enqueue the latest frame and dequeue the oldest frame\n",
    "    else:\n",
    "        stacked_frames.append(frame)\n",
    "        stacked_state = np.stack(stacked_frames, axis = 2)\n",
    "        \n",
    "    return stacked_state, stacked_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = [84, 84, 4]  # Size of input to the model\n",
    "\n",
    "action_size = game.get_available_buttons_size()\n",
    "\n",
    "learning_rate = 0.0002\n",
    "\n",
    "total_episodes = 500  # Total number of training games\n",
    "max_steps = 100  # Maximum steps to be taken in a game\n",
    "batch_size = 64  # Batch size input to the model\n",
    "\n",
    "epsilon_start = 1.  # max exploration rate\n",
    "epsilon_end = 0.01  # min exploration rate\n",
    "decay_rate = 0.0001  # decay rate per game\n",
    "\n",
    "gamma = 0.95   #discount factor\n",
    "\n",
    "pretrain_length = batch_size  # Initial memory size\n",
    "memory_size = 1000000  # Maximum memory size\n",
    "\n",
    "training = True  # Boolen value to train or not\n",
    "render = True  # Boolean value to see the agent train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacked_frame [84, 84, 4] ---> conv1 ----> [20, 20, 32] (batch_norm + elu) ----> conv2 ----> [9, 9, 64] (batch_norm + elu)\n",
    "# -----> conv3 -----> [3, 3, 128] (batch_norm + elu) ----> Flatten -----> [1152,] -----> Dense ----> [512,] (elu)\n",
    "# ------> Dense ----> [self.action,]\n",
    "\n",
    "class DQNetwork:\n",
    "    def __init__(self, state_size, action_size, learning_rate, name = 'DQNetwork'):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            # placeholders for inputs and actions\n",
    "            self.inputs_ = tf.placeholder(tf.float32, [None, *state_size], name = 'inputs')\n",
    "            self.actions_ = tf.placeholder(tf.float32, [None, 3], name = 'actions')\n",
    "            \n",
    "            # Placeholder of target variable\n",
    "            self.target_Q = tf.placeholder(tf.float32, [None], name = 'targets')\n",
    "            \n",
    "            self.conv1 = tf.layers.conv2d(inputs = self.inputs_,\n",
    "                                         filters = 32,\n",
    "                                         kernel_size = [8,8],\n",
    "                                         strides = [4,4],\n",
    "                                         padding = \"VALID\",\n",
    "                                          kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                         name = \"conv1\")\n",
    "            \n",
    "            self.conv1_batchnorm = tf.layers.batch_normalization(self.conv1,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm1')\n",
    "\n",
    "            self.conv1_out = tf.nn.elu(self.conv1_batchnorm, name=\"conv1_out\")\n",
    "            \n",
    "            self.conv2 = tf.layers.conv2d(inputs = self.conv1_out,\n",
    "                                 filters = 64,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv2\")\n",
    "        \n",
    "            self.conv2_batchnorm = tf.layers.batch_normalization(self.conv2,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm2')\n",
    "\n",
    "            self.conv2_out = tf.nn.elu(self.conv2_batchnorm, name=\"conv2_out\")\n",
    "            \n",
    "            self.conv3 = tf.layers.conv2d(inputs = self.conv2_out,\n",
    "                                 filters = 128,\n",
    "                                 kernel_size = [4,4],\n",
    "                                 strides = [2,2],\n",
    "                                 padding = \"VALID\",\n",
    "                                kernel_initializer=tf.contrib.layers.xavier_initializer_conv2d(),\n",
    "                                 name = \"conv3\")\n",
    "        \n",
    "            self.conv3_batchnorm = tf.layers.batch_normalization(self.conv3,\n",
    "                                                   training = True,\n",
    "                                                   epsilon = 1e-5,\n",
    "                                                     name = 'batch_norm3')\n",
    "\n",
    "            self.conv3_out = tf.nn.elu(self.conv3_batchnorm, name=\"conv3_out\")\n",
    "            \n",
    "            self.flatten = tf.layers.flatten(self.conv3_out)\n",
    "            \n",
    "            self.fc = tf.layers.dense(inputs = self.flatten,\n",
    "                                  units = 512,\n",
    "                                  activation = tf.nn.elu,\n",
    "                                       kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                name=\"fc1\")\n",
    "            \n",
    "            \n",
    "            self.output = tf.layers.dense(inputs = self.fc, \n",
    "                                           kernel_initializer=tf.contrib.layers.xavier_initializer(),\n",
    "                                          units = action_size, \n",
    "                                        activation=None)\n",
    "            \n",
    "            self.Q = tf.reduce_sum(tf.multiply(self.output, self.actions_), axis=1)\n",
    "            \n",
    "            # Mean squared loss function\n",
    "            self.loss = tf.reduce_mean(tf.square(self.target_Q - self.Q))\n",
    "            \n",
    "            # Adam optimizer\n",
    "            self.optimizer = tf.train.AdamOptimizer(self.learning_rate).minimize(self.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002D389B6EEC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002D389B6EEC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002D389B6EEC8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002D389B6EEC8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002D38A36F748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002D38A36F748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002D38A36F748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002D38A36F748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002D38A36F748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002D38A36F748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002D38A36F748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002D38A36F748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002D3F0328808>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002D3F0328808>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002D3F0328808>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002D3F0328808>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002D389932248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002D389932248>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002D389932248>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Conv.call of <tensorflow.python.layers.convolutional.Conv2D object at 0x000002D389932248>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002D3FB3D6908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002D3FB3D6908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002D3FB3D6908>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x000002D3FB3D6908>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002D3FB722748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002D3FB722748>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002D3FB722748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x000002D3FB722748>>: AttributeError: module 'gast' has no attribute 'Num'\n",
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002D3FB722748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002D3FB722748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002D3FB722748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002D3FB722748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002D3FB722748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002D3FB722748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002D3FB722748>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Dense.call of <tensorflow.python.layers.core.Dense object at 0x000002D3FB722748>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "# reset the tensorflow graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Initialize network\n",
    "DQNetwork = DQNetwork(state_size, action_size, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience Replay\n",
    "\n",
    "Memory class to store experience (stacked frames, action, reward, next state, boolean to know game finished or not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory():\n",
    "    def __init__(self, max_size):\n",
    "        self.buffer = deque(maxlen = max_size)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.buffer.append(experience)\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        buffer_size = len(self.buffer)\n",
    "        \n",
    "        index = np.random.choice(np.arange(buffer_size), size = batch_size,\n",
    "                                replace = False)\n",
    "        \n",
    "        return [self.buffer[i] for i in index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially filling memory with random experiences from the start frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = Memory(max_size = memory_size)\n",
    "\n",
    "game.new_episode()\n",
    "\n",
    "for i in range(pretrain_length):\n",
    "    if i==0:\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state,\n",
    "                                            True)\n",
    "        \n",
    "    action = random.choice(possible_actions)\n",
    "    \n",
    "    reward = game.make_action(action)\n",
    "    \n",
    "    done = game.is_episode_finished()\n",
    "    \n",
    "    if done:\n",
    "        next_state = np.zeros(state.shape)   # all zeros mark the final state\n",
    "        \n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        game.new_episode()\n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames, state, True)\n",
    "        \n",
    "    else:\n",
    "        next_state = game.get_state().screen_buffer\n",
    "        next_state, stacked_frames = stack_frames(stacked_frames,\n",
    "                                                 next_state, False)\n",
    "        \n",
    "        memory.add((state, action, reward, next_state, done))\n",
    "        \n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorboard Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = tf.summary.FileWriter(\"/tensorboard/dqn/1\")\n",
    "\n",
    "tf.summary.scalar('loss', DQNetwork.loss)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_action(epsilon_start, epsilon_end, decay_rate, decay_step, state, actions):\n",
    "    exp_exp_tradeoff = np.random.rand()\n",
    "    explore_probability = epsilon_end + (epsilon_start - epsilon_end)*np.exp(-decay_rate*decay_step)\n",
    "    \n",
    "    if explore_probability > exp_exp_tradeoff:\n",
    "        action = random.choice(possible_actions)\n",
    "    \n",
    "    else:\n",
    "        Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "        \n",
    "        choice = np.argmax(Qs)\n",
    "        action = possible_actions[choice]\n",
    "        \n",
    "    return action, explore_probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "Episode: 1 Total reward: 95.0 Training loss: 2.8436 Explore Prob: 0.9994\n",
      "Episode: 2 Total reward: 93.0 Training loss: 2.5919 Explore Prob: 0.9986\n",
      "Model Saved\n",
      "Episode: 3 Total reward: 90.0 Training loss: 8.6603 Explore Prob: 0.9975\n",
      "Model Saved\n",
      "Episode: 5 Total reward: 95.0 Training loss: 2.5018 Explore Prob: 0.9871\n",
      "Model Saved\n",
      "Episode: 7 Total reward: 92.0 Training loss: 1.0611 Explore Prob: 0.9765\n",
      "Episode: 8 Total reward: 71.0 Training loss: 11.4534 Explore Prob: 0.9741\n",
      "Model Saved\n",
      "Episode: 10 Total reward: 88.0 Training loss: 1.5417 Explore Prob: 0.9633\n",
      "Model Saved\n",
      "Episode: 11 Total reward: 3.0 Training loss: 3.4974 Explore Prob: 0.9559\n",
      "Episode: 12 Total reward: 35.0 Training loss: 3.8786 Explore Prob: 0.9506\n",
      "Model Saved\n",
      "Episode: 13 Total reward: 95.0 Training loss: 3.0608 Explore Prob: 0.9500\n",
      "Model Saved\n",
      "Episode: 16 Total reward: 92.0 Training loss: 1.0314 Explore Prob: 0.9306\n",
      "Model Saved\n",
      "Model Saved\n",
      "Episode: 20 Total reward: 94.0 Training loss: 1.2340 Explore Prob: 0.9027\n",
      "Model Saved\n",
      "Episode: 22 Total reward: 32.0 Training loss: 2.0173 Explore Prob: 0.8891\n",
      "Model Saved\n",
      "Episode: 23 Total reward: 94.0 Training loss: 2.0818 Explore Prob: 0.8885\n",
      "Episode: 24 Total reward: 36.0 Training loss: 1.4226 Explore Prob: 0.8837\n",
      "Model Saved\n",
      "Episode: 25 Total reward: 93.0 Training loss: 2.3369 Explore Prob: 0.8830\n",
      "Episode: 26 Total reward: -16.0 Training loss: 1.8893 Explore Prob: 0.8750\n",
      "Model Saved\n",
      "Episode: 28 Total reward: 94.0 Training loss: 0.6969 Explore Prob: 0.8658\n",
      "Model Saved\n",
      "Episode: 29 Total reward: 95.0 Training loss: 0.8542 Explore Prob: 0.8653\n",
      "Episode: 30 Total reward: 69.0 Training loss: 1.5806 Explore Prob: 0.8630\n",
      "Model Saved\n",
      "Episode: 32 Total reward: 90.0 Training loss: 3.6626 Explore Prob: 0.8535\n",
      "Model Saved\n",
      "Episode: 33 Total reward: 63.0 Training loss: 3.1416 Explore Prob: 0.8508\n",
      "Episode: 34 Total reward: 95.0 Training loss: 1.0721 Explore Prob: 0.8503\n",
      "Model Saved\n",
      "Episode: 36 Total reward: 95.0 Training loss: 3.0266 Explore Prob: 0.8414\n",
      "Model Saved\n",
      "Episode: 38 Total reward: 93.0 Training loss: 1.7573 Explore Prob: 0.8325\n",
      "Model Saved\n",
      "Episode: 39 Total reward: 93.0 Training loss: 2.0725 Explore Prob: 0.8318\n",
      "Episode: 40 Total reward: 56.0 Training loss: 1.2629 Explore Prob: 0.8289\n",
      "Model Saved\n",
      "Episode: 41 Total reward: -11.0 Training loss: 3.0137 Explore Prob: 0.8214\n",
      "Episode: 42 Total reward: 95.0 Training loss: 1.5634 Explore Prob: 0.8209\n",
      "Model Saved\n",
      "Episode: 43 Total reward: 19.0 Training loss: 4.7111 Explore Prob: 0.8155\n",
      "Episode: 44 Total reward: 93.0 Training loss: 2.2739 Explore Prob: 0.8149\n",
      "Model Saved\n",
      "Episode: 45 Total reward: 95.0 Training loss: 5.4653 Explore Prob: 0.8144\n",
      "Episode: 46 Total reward: 16.0 Training loss: 2.9124 Explore Prob: 0.8088\n",
      "Model Saved\n",
      "Episode: 47 Total reward: 95.0 Training loss: 3.7873 Explore Prob: 0.8083\n",
      "Episode: 48 Total reward: 75.0 Training loss: 3.5886 Explore Prob: 0.8066\n",
      "Model Saved\n",
      "Episode: 49 Total reward: 95.0 Training loss: 2.5979 Explore Prob: 0.8062\n",
      "Episode: 50 Total reward: 95.0 Training loss: 2.2704 Explore Prob: 0.8057\n",
      "Model Saved\n",
      "Episode: 51 Total reward: 93.0 Training loss: 2.5159 Explore Prob: 0.8050\n",
      "Episode: 52 Total reward: 94.0 Training loss: 6.8078 Explore Prob: 0.8045\n",
      "Model Saved\n",
      "Episode: 53 Total reward: 42.0 Training loss: 6.4207 Explore Prob: 0.8006\n",
      "Episode: 54 Total reward: 95.0 Training loss: 5.4087 Explore Prob: 0.8001\n",
      "Model Saved\n",
      "Episode: 55 Total reward: 95.0 Training loss: 8.6166 Explore Prob: 0.7997\n",
      "Episode: 56 Total reward: 66.0 Training loss: 4.3026 Explore Prob: 0.7973\n",
      "Model Saved\n",
      "Episode: 57 Total reward: 93.0 Training loss: 4.3358 Explore Prob: 0.7967\n",
      "Episode: 58 Total reward: 95.0 Training loss: 4.1279 Explore Prob: 0.7962\n",
      "Model Saved\n",
      "Episode: 59 Total reward: 93.0 Training loss: 5.1919 Explore Prob: 0.7956\n",
      "Episode: 60 Total reward: 95.0 Training loss: 5.8222 Explore Prob: 0.7951\n",
      "Model Saved\n",
      "Episode: 61 Total reward: 17.0 Training loss: 2.2533 Explore Prob: 0.7897\n",
      "Model Saved\n",
      "Episode: 63 Total reward: 93.0 Training loss: 6.4098 Explore Prob: 0.7813\n",
      "Episode: 64 Total reward: 94.0 Training loss: 3.3433 Explore Prob: 0.7808\n",
      "Model Saved\n",
      "Episode: 65 Total reward: 95.0 Training loss: 4.6958 Explore Prob: 0.7803\n",
      "Episode: 66 Total reward: 93.0 Training loss: 2.1392 Explore Prob: 0.7797\n",
      "Model Saved\n",
      "Episode: 67 Total reward: 95.0 Training loss: 2.2163 Explore Prob: 0.7792\n",
      "Episode: 68 Total reward: 21.0 Training loss: 1.5348 Explore Prob: 0.7743\n",
      "Model Saved\n",
      "Episode: 69 Total reward: 95.0 Training loss: 2.0209 Explore Prob: 0.7738\n",
      "Episode: 70 Total reward: 95.0 Training loss: 4.6025 Explore Prob: 0.7733\n",
      "Model Saved\n",
      "Episode: 71 Total reward: 17.0 Training loss: 3.2319 Explore Prob: 0.7681\n",
      "Model Saved\n",
      "Episode: 73 Total reward: -2.0 Training loss: 1.4506 Explore Prob: 0.7543\n",
      "Episode: 74 Total reward: 93.0 Training loss: 1.7665 Explore Prob: 0.7537\n",
      "Model Saved\n",
      "Episode: 75 Total reward: 95.0 Training loss: 1.9423 Explore Prob: 0.7533\n",
      "Episode: 76 Total reward: 95.0 Training loss: 2.4239 Explore Prob: 0.7529\n",
      "Model Saved\n",
      "Episode: 77 Total reward: 57.0 Training loss: 2.9655 Explore Prob: 0.7503\n",
      "Episode: 78 Total reward: 7.0 Training loss: 4.9185 Explore Prob: 0.7445\n",
      "Model Saved\n",
      "Episode: 79 Total reward: 38.0 Training loss: 1.7109 Explore Prob: 0.7406\n",
      "Episode: 80 Total reward: 70.0 Training loss: 4.7849 Explore Prob: 0.7387\n",
      "Model Saved\n",
      "Episode: 81 Total reward: 14.0 Training loss: 3.7901 Explore Prob: 0.7335\n",
      "Episode: 82 Total reward: 33.0 Training loss: 2.9453 Explore Prob: 0.7293\n",
      "Model Saved\n",
      "Episode: 83 Total reward: 95.0 Training loss: 3.0410 Explore Prob: 0.7289\n",
      "Episode: 84 Total reward: 88.0 Training loss: 2.1127 Explore Prob: 0.7280\n",
      "Model Saved\n",
      "Episode: 85 Total reward: 10.0 Training loss: 3.9463 Explore Prob: 0.7222\n",
      "Episode: 86 Total reward: 95.0 Training loss: 2.2339 Explore Prob: 0.7217\n",
      "Model Saved\n",
      "Episode: 87 Total reward: 94.0 Training loss: 1.6506 Explore Prob: 0.7212\n",
      "Episode: 88 Total reward: 17.0 Training loss: 1.4707 Explore Prob: 0.7163\n",
      "Model Saved\n",
      "Episode: 89 Total reward: 95.0 Training loss: 1.7372 Explore Prob: 0.7159\n",
      "Episode: 90 Total reward: 88.0 Training loss: 2.7000 Explore Prob: 0.7150\n",
      "Model Saved\n",
      "Episode: 91 Total reward: -8.0 Training loss: 2.8600 Explore Prob: 0.7088\n",
      "Episode: 92 Total reward: 14.0 Training loss: 5.7599 Explore Prob: 0.7037\n",
      "Model Saved\n",
      "Episode: 93 Total reward: 95.0 Training loss: 2.3518 Explore Prob: 0.7033\n",
      "Model Saved\n",
      "Episode: 95 Total reward: 94.0 Training loss: 26.1786 Explore Prob: 0.6960\n",
      "Episode: 96 Total reward: 94.0 Training loss: 2.6253 Explore Prob: 0.6955\n",
      "Model Saved\n",
      "Episode: 97 Total reward: 94.0 Training loss: 2.9789 Explore Prob: 0.6950\n",
      "Episode: 98 Total reward: 65.0 Training loss: 2.9670 Explore Prob: 0.6929\n",
      "Model Saved\n",
      "Episode: 99 Total reward: 23.0 Training loss: 5.3992 Explore Prob: 0.6886\n",
      "Episode: 100 Total reward: 94.0 Training loss: 1.7422 Explore Prob: 0.6881\n",
      "Model Saved\n",
      "Episode: 101 Total reward: 70.0 Training loss: 2.4604 Explore Prob: 0.6863\n",
      "Episode: 102 Total reward: 95.0 Training loss: 6.1306 Explore Prob: 0.6859\n",
      "Model Saved\n",
      "Episode: 103 Total reward: 95.0 Training loss: 5.3072 Explore Prob: 0.6855\n",
      "Episode: 104 Total reward: 94.0 Training loss: 3.2607 Explore Prob: 0.6851\n",
      "Model Saved\n",
      "Episode: 105 Total reward: 91.0 Training loss: 3.0905 Explore Prob: 0.6844\n",
      "Episode: 106 Total reward: 95.0 Training loss: 7.5324 Explore Prob: 0.6840\n",
      "Model Saved\n",
      "Episode: 107 Total reward: 53.0 Training loss: 1.4469 Explore Prob: 0.6814\n",
      "Episode: 108 Total reward: 23.0 Training loss: 2.6532 Explore Prob: 0.6772\n",
      "Model Saved\n",
      "Episode: 109 Total reward: 95.0 Training loss: 1.9595 Explore Prob: 0.6768\n",
      "Episode: 110 Total reward: 13.0 Training loss: 5.1878 Explore Prob: 0.6720\n",
      "Model Saved\n",
      "Episode: 111 Total reward: 73.0 Training loss: 3.6012 Explore Prob: 0.6704\n",
      "Episode: 112 Total reward: 95.0 Training loss: 5.4828 Explore Prob: 0.6700\n",
      "Model Saved\n",
      "Episode: 113 Total reward: 39.0 Training loss: 2.6411 Explore Prob: 0.6666\n",
      "Episode: 114 Total reward: 37.0 Training loss: 17.3215 Explore Prob: 0.6634\n",
      "Model Saved\n",
      "Episode: 115 Total reward: 56.0 Training loss: 9.7918 Explore Prob: 0.6611\n",
      "Episode: 116 Total reward: -4.0 Training loss: 5.7032 Explore Prob: 0.6556\n",
      "Model Saved\n",
      "Episode: 117 Total reward: -8.0 Training loss: 19.6301 Explore Prob: 0.6499\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 118 Total reward: 94.0 Training loss: 4.8631 Explore Prob: 0.6494\n",
      "Model Saved\n",
      "Episode: 119 Total reward: 92.0 Training loss: 9.4872 Explore Prob: 0.6489\n",
      "Episode: 120 Total reward: 95.0 Training loss: 3.4232 Explore Prob: 0.6485\n",
      "Model Saved\n",
      "Episode: 121 Total reward: 94.0 Training loss: 5.7635 Explore Prob: 0.6480\n",
      "Episode: 122 Total reward: 95.0 Training loss: 4.2862 Explore Prob: 0.6477\n",
      "Model Saved\n",
      "Episode: 123 Total reward: 95.0 Training loss: 3.7789 Explore Prob: 0.6473\n",
      "Episode: 124 Total reward: 95.0 Training loss: 5.2755 Explore Prob: 0.6469\n",
      "Model Saved\n",
      "Episode: 125 Total reward: 94.0 Training loss: 7.9627 Explore Prob: 0.6464\n",
      "Episode: 126 Total reward: 95.0 Training loss: 3.9292 Explore Prob: 0.6461\n",
      "Model Saved\n",
      "Episode: 127 Total reward: 94.0 Training loss: 7.9246 Explore Prob: 0.6456\n",
      "Episode: 128 Total reward: 95.0 Training loss: 2.2060 Explore Prob: 0.6452\n",
      "Model Saved\n",
      "Episode: 129 Total reward: 40.0 Training loss: 2.1900 Explore Prob: 0.6420\n",
      "Episode: 130 Total reward: 95.0 Training loss: 2.5474 Explore Prob: 0.6416\n",
      "Model Saved\n",
      "Episode: 131 Total reward: 69.0 Training loss: 7.5692 Explore Prob: 0.6399\n",
      "Episode: 132 Total reward: 94.0 Training loss: 4.4789 Explore Prob: 0.6395\n",
      "Model Saved\n",
      "Episode: 133 Total reward: 95.0 Training loss: 9.1719 Explore Prob: 0.6391\n",
      "Episode: 134 Total reward: 95.0 Training loss: 4.3296 Explore Prob: 0.6387\n",
      "Model Saved\n",
      "Episode: 135 Total reward: 94.0 Training loss: 7.2284 Explore Prob: 0.6383\n",
      "Episode: 136 Total reward: 76.0 Training loss: 8.8079 Explore Prob: 0.6370\n",
      "Model Saved\n",
      "Episode: 137 Total reward: 92.0 Training loss: 3.2876 Explore Prob: 0.6365\n",
      "Episode: 138 Total reward: 33.0 Training loss: 3.2089 Explore Prob: 0.6328\n",
      "Model Saved\n",
      "Episode: 139 Total reward: 90.0 Training loss: 2.1435 Explore Prob: 0.6322\n",
      "Episode: 140 Total reward: 31.0 Training loss: 2.8263 Explore Prob: 0.6288\n",
      "Model Saved\n",
      "Episode: 141 Total reward: 95.0 Training loss: 5.3209 Explore Prob: 0.6284\n",
      "Episode: 142 Total reward: 48.0 Training loss: 5.2338 Explore Prob: 0.6257\n",
      "Model Saved\n",
      "Episode: 143 Total reward: 94.0 Training loss: 3.3679 Explore Prob: 0.6253\n",
      "Episode: 144 Total reward: 95.0 Training loss: 5.9204 Explore Prob: 0.6249\n",
      "Model Saved\n",
      "Episode: 145 Total reward: 24.0 Training loss: 4.3362 Explore Prob: 0.6211\n",
      "Episode: 146 Total reward: 72.0 Training loss: 1.6673 Explore Prob: 0.6197\n",
      "Model Saved\n",
      "Episode: 147 Total reward: -16.0 Training loss: 1.8398 Explore Prob: 0.6138\n",
      "Episode: 148 Total reward: 18.0 Training loss: 1.7854 Explore Prob: 0.6097\n",
      "Model Saved\n",
      "Episode: 149 Total reward: 94.0 Training loss: 2.5081 Explore Prob: 0.6093\n",
      "Episode: 150 Total reward: 95.0 Training loss: 2.6618 Explore Prob: 0.6089\n",
      "Model Saved\n",
      "Episode: 151 Total reward: 22.0 Training loss: 3.1297 Explore Prob: 0.6051\n",
      "Model Saved\n",
      "Episode: 153 Total reward: 95.0 Training loss: 5.0509 Explore Prob: 0.5988\n",
      "Episode: 154 Total reward: 95.0 Training loss: 9.5679 Explore Prob: 0.5985\n",
      "Model Saved\n",
      "Episode: 155 Total reward: 61.0 Training loss: 4.4417 Explore Prob: 0.5964\n",
      "Episode: 156 Total reward: 38.0 Training loss: 3.6846 Explore Prob: 0.5936\n",
      "Model Saved\n",
      "Episode: 157 Total reward: 94.0 Training loss: 3.0832 Explore Prob: 0.5932\n",
      "Episode: 158 Total reward: 94.0 Training loss: 3.6186 Explore Prob: 0.5928\n",
      "Model Saved\n",
      "Episode: 159 Total reward: 94.0 Training loss: 6.3289 Explore Prob: 0.5924\n",
      "Episode: 160 Total reward: 95.0 Training loss: 5.0721 Explore Prob: 0.5920\n",
      "Model Saved\n",
      "Episode: 161 Total reward: 95.0 Training loss: 2.7061 Explore Prob: 0.5917\n",
      "Episode: 162 Total reward: 38.0 Training loss: 1.7951 Explore Prob: 0.5886\n",
      "Model Saved\n",
      "Episode: 163 Total reward: 94.0 Training loss: 3.3536 Explore Prob: 0.5882\n",
      "Episode: 164 Total reward: 70.0 Training loss: 2.9824 Explore Prob: 0.5867\n",
      "Model Saved\n",
      "Episode: 165 Total reward: 95.0 Training loss: 3.5533 Explore Prob: 0.5863\n",
      "Episode: 166 Total reward: 6.0 Training loss: 4.4055 Explore Prob: 0.5820\n",
      "Model Saved\n",
      "Episode: 167 Total reward: 93.0 Training loss: 4.2408 Explore Prob: 0.5816\n",
      "Episode: 168 Total reward: 2.0 Training loss: 4.2776 Explore Prob: 0.5771\n",
      "Model Saved\n",
      "Episode: 169 Total reward: 44.0 Training loss: 4.9221 Explore Prob: 0.5744\n",
      "Episode: 170 Total reward: 95.0 Training loss: 3.9137 Explore Prob: 0.5741\n",
      "Model Saved\n",
      "Episode: 171 Total reward: 14.0 Training loss: 39.0824 Explore Prob: 0.5703\n",
      "Episode: 172 Total reward: 95.0 Training loss: 8.4340 Explore Prob: 0.5700\n",
      "Model Saved\n",
      "Episode: 173 Total reward: -1.0 Training loss: 3.5434 Explore Prob: 0.5654\n",
      "Episode: 174 Total reward: 3.0 Training loss: 9.3264 Explore Prob: 0.5611\n",
      "Model Saved\n",
      "Episode: 175 Total reward: 94.0 Training loss: 5.4775 Explore Prob: 0.5607\n",
      "Episode: 176 Total reward: 95.0 Training loss: 4.7549 Explore Prob: 0.5604\n",
      "Model Saved\n",
      "Episode: 177 Total reward: 95.0 Training loss: 3.4811 Explore Prob: 0.5600\n",
      "Episode: 178 Total reward: 95.0 Training loss: 3.9410 Explore Prob: 0.5597\n",
      "Model Saved\n",
      "Episode: 179 Total reward: 95.0 Training loss: 4.4530 Explore Prob: 0.5594\n",
      "Episode: 180 Total reward: 95.0 Training loss: 7.2654 Explore Prob: 0.5591\n",
      "Model Saved\n",
      "Episode: 182 Total reward: 70.0 Training loss: 5.9293 Explore Prob: 0.5522\n",
      "Model Saved\n",
      "Episode: 183 Total reward: 95.0 Training loss: 4.0535 Explore Prob: 0.5519\n",
      "Episode: 184 Total reward: 95.0 Training loss: 3.1225 Explore Prob: 0.5515\n",
      "Model Saved\n",
      "Episode: 185 Total reward: 33.0 Training loss: 3.3218 Explore Prob: 0.5487\n",
      "Episode: 186 Total reward: 68.0 Training loss: 2.6834 Explore Prob: 0.5472\n",
      "Model Saved\n",
      "Episode: 188 Total reward: 94.0 Training loss: 4.1601 Explore Prob: 0.5414\n",
      "Model Saved\n",
      "Episode: 189 Total reward: 21.0 Training loss: 6.0466 Explore Prob: 0.5380\n",
      "Episode: 190 Total reward: 95.0 Training loss: 5.3224 Explore Prob: 0.5377\n",
      "Model Saved\n",
      "Episode: 191 Total reward: 95.0 Training loss: 2.6214 Explore Prob: 0.5374\n",
      "Episode: 192 Total reward: 95.0 Training loss: 2.7764 Explore Prob: 0.5371\n",
      "Model Saved\n",
      "Episode: 193 Total reward: 73.0 Training loss: 7.1062 Explore Prob: 0.5358\n",
      "Episode: 194 Total reward: 95.0 Training loss: 4.1407 Explore Prob: 0.5355\n",
      "Model Saved\n",
      "Episode: 195 Total reward: 94.0 Training loss: 3.2174 Explore Prob: 0.5352\n",
      "Episode: 196 Total reward: 67.0 Training loss: 39.9913 Explore Prob: 0.5336\n",
      "Model Saved\n",
      "Episode: 197 Total reward: 95.0 Training loss: 7.4951 Explore Prob: 0.5333\n",
      "Episode: 198 Total reward: 95.0 Training loss: 8.2802 Explore Prob: 0.5330\n",
      "Model Saved\n",
      "Episode: 199 Total reward: 95.0 Training loss: 4.7479 Explore Prob: 0.5327\n",
      "Episode: 200 Total reward: 95.0 Training loss: 5.6154 Explore Prob: 0.5324\n",
      "Model Saved\n",
      "Episode: 201 Total reward: -11.0 Training loss: 3.2983 Explore Prob: 0.5276\n",
      "Episode: 202 Total reward: 95.0 Training loss: 2.2740 Explore Prob: 0.5273\n",
      "Model Saved\n",
      "Episode: 203 Total reward: 48.0 Training loss: 3.2799 Explore Prob: 0.5251\n",
      "Episode: 204 Total reward: 95.0 Training loss: 2.0695 Explore Prob: 0.5248\n",
      "Model Saved\n",
      "Episode: 205 Total reward: 41.0 Training loss: 4.1012 Explore Prob: 0.5222\n",
      "Episode: 206 Total reward: 53.0 Training loss: 6.1810 Explore Prob: 0.5200\n",
      "Model Saved\n",
      "Episode: 207 Total reward: 30.0 Training loss: 2.7498 Explore Prob: 0.5169\n",
      "Episode: 208 Total reward: 42.0 Training loss: 4.2921 Explore Prob: 0.5144\n",
      "Model Saved\n",
      "Episode: 209 Total reward: 95.0 Training loss: 39.0620 Explore Prob: 0.5141\n",
      "Episode: 210 Total reward: 94.0 Training loss: 4.8396 Explore Prob: 0.5138\n",
      "Model Saved\n",
      "Episode: 211 Total reward: 95.0 Training loss: 3.4367 Explore Prob: 0.5135\n",
      "Episode: 212 Total reward: 95.0 Training loss: 6.4093 Explore Prob: 0.5132\n",
      "Model Saved\n",
      "Episode: 213 Total reward: 95.0 Training loss: 2.4885 Explore Prob: 0.5129\n",
      "Episode: 214 Total reward: 70.0 Training loss: 4.7504 Explore Prob: 0.5116\n",
      "Model Saved\n",
      "Episode: 215 Total reward: 94.0 Training loss: 4.4835 Explore Prob: 0.5112\n",
      "Episode: 216 Total reward: 51.0 Training loss: 5.0019 Explore Prob: 0.5092\n",
      "Model Saved\n",
      "Episode: 217 Total reward: 70.0 Training loss: 3.9589 Explore Prob: 0.5079\n",
      "Episode: 218 Total reward: 95.0 Training loss: 2.8899 Explore Prob: 0.5076\n",
      "Model Saved\n",
      "Episode: 219 Total reward: 95.0 Training loss: 5.1444 Explore Prob: 0.5073\n",
      "Episode: 220 Total reward: 46.0 Training loss: 2.9185 Explore Prob: 0.5051\n",
      "Model Saved\n",
      "Episode: 221 Total reward: 95.0 Training loss: 8.7388 Explore Prob: 0.5048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 222 Total reward: 71.0 Training loss: 8.9301 Explore Prob: 0.5035\n",
      "Model Saved\n",
      "Episode: 223 Total reward: 44.0 Training loss: 5.1608 Explore Prob: 0.5012\n",
      "Episode: 224 Total reward: 95.0 Training loss: 4.4062 Explore Prob: 0.5009\n",
      "Model Saved\n",
      "Episode: 225 Total reward: 70.0 Training loss: 5.2935 Explore Prob: 0.4997\n",
      "Episode: 226 Total reward: 95.0 Training loss: 7.1209 Explore Prob: 0.4994\n",
      "Model Saved\n",
      "Episode: 227 Total reward: 95.0 Training loss: 4.4838 Explore Prob: 0.4991\n",
      "Episode: 228 Total reward: 28.0 Training loss: 3.1025 Explore Prob: 0.4960\n",
      "Model Saved\n",
      "Episode: 229 Total reward: 95.0 Training loss: 5.6088 Explore Prob: 0.4957\n",
      "Episode: 230 Total reward: 75.0 Training loss: 2.4912 Explore Prob: 0.4947\n",
      "Model Saved\n",
      "Episode: 231 Total reward: 92.0 Training loss: 5.5611 Explore Prob: 0.4943\n",
      "Episode: 232 Total reward: 92.0 Training loss: 3.7123 Explore Prob: 0.4938\n",
      "Model Saved\n",
      "Episode: 233 Total reward: 95.0 Training loss: 7.3406 Explore Prob: 0.4935\n",
      "Episode: 234 Total reward: 13.0 Training loss: 11.2787 Explore Prob: 0.4900\n",
      "Model Saved\n",
      "Episode: 235 Total reward: 94.0 Training loss: 3.4391 Explore Prob: 0.4897\n",
      "Episode: 236 Total reward: 95.0 Training loss: 5.6948 Explore Prob: 0.4894\n",
      "Model Saved\n",
      "Episode: 237 Total reward: 69.0 Training loss: 2.9079 Explore Prob: 0.4881\n",
      "Episode: 238 Total reward: 94.0 Training loss: 5.4946 Explore Prob: 0.4878\n",
      "Model Saved\n",
      "Episode: 239 Total reward: 87.0 Training loss: 1.9718 Explore Prob: 0.4871\n",
      "Episode: 240 Total reward: 67.0 Training loss: 2.9662 Explore Prob: 0.4857\n",
      "Model Saved\n",
      "Episode: 241 Total reward: 64.0 Training loss: 5.3201 Explore Prob: 0.4842\n",
      "Episode: 242 Total reward: 95.0 Training loss: 6.9265 Explore Prob: 0.4839\n",
      "Model Saved\n",
      "Episode: 243 Total reward: 95.0 Training loss: 2.7042 Explore Prob: 0.4836\n",
      "Episode: 244 Total reward: 95.0 Training loss: 4.7935 Explore Prob: 0.4833\n",
      "Model Saved\n",
      "Episode: 245 Total reward: 94.0 Training loss: 2.0104 Explore Prob: 0.4830\n",
      "Episode: 246 Total reward: 95.0 Training loss: 2.2023 Explore Prob: 0.4827\n",
      "Model Saved\n",
      "Episode: 247 Total reward: 95.0 Training loss: 2.0321 Explore Prob: 0.4824\n",
      "Episode: 248 Total reward: 95.0 Training loss: 2.4969 Explore Prob: 0.4822\n",
      "Model Saved\n",
      "Episode: 249 Total reward: 11.0 Training loss: 2.8643 Explore Prob: 0.4786\n",
      "Episode: 250 Total reward: 19.0 Training loss: 6.4110 Explore Prob: 0.4755\n",
      "Model Saved\n",
      "Episode: 251 Total reward: 71.0 Training loss: 12.3410 Explore Prob: 0.4743\n",
      "Episode: 252 Total reward: 61.0 Training loss: 3.8218 Explore Prob: 0.4727\n",
      "Model Saved\n",
      "Episode: 253 Total reward: 95.0 Training loss: 3.8608 Explore Prob: 0.4724\n",
      "Episode: 254 Total reward: 95.0 Training loss: 17.6433 Explore Prob: 0.4722\n",
      "Model Saved\n",
      "Episode: 255 Total reward: 95.0 Training loss: 4.8609 Explore Prob: 0.4719\n",
      "Episode: 256 Total reward: 95.0 Training loss: 8.5264 Explore Prob: 0.4716\n",
      "Model Saved\n",
      "Episode: 257 Total reward: 52.0 Training loss: 2.6477 Explore Prob: 0.4696\n",
      "Episode: 258 Total reward: 95.0 Training loss: 4.6931 Explore Prob: 0.4693\n",
      "Model Saved\n",
      "Episode: 259 Total reward: 39.0 Training loss: 1.7926 Explore Prob: 0.4669\n",
      "Episode: 260 Total reward: 95.0 Training loss: 2.2549 Explore Prob: 0.4666\n",
      "Model Saved\n",
      "Episode: 261 Total reward: 95.0 Training loss: 3.2396 Explore Prob: 0.4664\n",
      "Episode: 262 Total reward: 94.0 Training loss: 17.3176 Explore Prob: 0.4661\n",
      "Model Saved\n",
      "Episode: 263 Total reward: 95.0 Training loss: 4.3062 Explore Prob: 0.4658\n",
      "Episode: 264 Total reward: 67.0 Training loss: 2.7327 Explore Prob: 0.4645\n",
      "Model Saved\n",
      "Episode: 265 Total reward: 95.0 Training loss: 2.2581 Explore Prob: 0.4642\n",
      "Episode: 266 Total reward: 95.0 Training loss: 5.9909 Explore Prob: 0.4639\n",
      "Model Saved\n",
      "Episode: 267 Total reward: 33.0 Training loss: 5.4997 Explore Prob: 0.4613\n",
      "Episode: 268 Total reward: 45.0 Training loss: 2.5496 Explore Prob: 0.4592\n",
      "Model Saved\n",
      "Episode: 269 Total reward: 95.0 Training loss: 1.9540 Explore Prob: 0.4589\n",
      "Episode: 270 Total reward: 95.0 Training loss: 2.0921 Explore Prob: 0.4587\n",
      "Model Saved\n",
      "Episode: 271 Total reward: 95.0 Training loss: 1.9778 Explore Prob: 0.4584\n",
      "Episode: 272 Total reward: 30.0 Training loss: 2.5655 Explore Prob: 0.4557\n",
      "Model Saved\n",
      "Episode: 273 Total reward: 79.0 Training loss: 5.1688 Explore Prob: 0.4547\n",
      "Episode: 274 Total reward: 23.0 Training loss: 3.4522 Explore Prob: 0.4519\n",
      "Model Saved\n",
      "Episode: 275 Total reward: 87.0 Training loss: 3.3502 Explore Prob: 0.4513\n",
      "Episode: 276 Total reward: 32.0 Training loss: 2.8502 Explore Prob: 0.4487\n",
      "Model Saved\n",
      "Episode: 277 Total reward: 48.0 Training loss: 3.6525 Explore Prob: 0.4468\n",
      "Episode: 278 Total reward: 95.0 Training loss: 4.9675 Explore Prob: 0.4466\n",
      "Model Saved\n",
      "Episode: 279 Total reward: 94.0 Training loss: 4.7576 Explore Prob: 0.4462\n",
      "Episode: 280 Total reward: 71.0 Training loss: 2.7550 Explore Prob: 0.4452\n",
      "Model Saved\n",
      "Episode: 281 Total reward: 95.0 Training loss: 3.1425 Explore Prob: 0.4449\n",
      "Episode: 282 Total reward: 95.0 Training loss: 3.6444 Explore Prob: 0.4446\n",
      "Model Saved\n",
      "Episode: 283 Total reward: 95.0 Training loss: 3.4045 Explore Prob: 0.4444\n",
      "Episode: 284 Total reward: 73.0 Training loss: 4.4659 Explore Prob: 0.4434\n",
      "Model Saved\n",
      "Episode: 285 Total reward: 95.0 Training loss: 6.6476 Explore Prob: 0.4431\n",
      "Episode: 286 Total reward: 95.0 Training loss: 3.2988 Explore Prob: 0.4429\n",
      "Model Saved\n",
      "Episode: 287 Total reward: 64.0 Training loss: 4.3192 Explore Prob: 0.4415\n",
      "Episode: 288 Total reward: 40.0 Training loss: 3.7844 Explore Prob: 0.4393\n",
      "Model Saved\n",
      "Episode: 289 Total reward: 49.0 Training loss: 2.1959 Explore Prob: 0.4375\n",
      "Episode: 290 Total reward: 94.0 Training loss: 1.3393 Explore Prob: 0.4372\n",
      "Model Saved\n",
      "Episode: 291 Total reward: 69.0 Training loss: 3.2545 Explore Prob: 0.4360\n",
      "Episode: 292 Total reward: 44.0 Training loss: 3.0357 Explore Prob: 0.4340\n",
      "Model Saved\n",
      "Episode: 293 Total reward: 95.0 Training loss: 6.5676 Explore Prob: 0.4338\n",
      "Episode: 294 Total reward: 44.0 Training loss: 3.9885 Explore Prob: 0.4318\n",
      "Model Saved\n",
      "Episode: 295 Total reward: 48.0 Training loss: 4.8481 Explore Prob: 0.4300\n",
      "Episode: 296 Total reward: 49.0 Training loss: 6.2280 Explore Prob: 0.4282\n",
      "Model Saved\n",
      "Episode: 297 Total reward: 76.0 Training loss: 6.3826 Explore Prob: 0.4274\n",
      "Episode: 298 Total reward: 95.0 Training loss: 1.5492 Explore Prob: 0.4271\n",
      "Model Saved\n",
      "Episode: 299 Total reward: 39.0 Training loss: 3.6282 Explore Prob: 0.4250\n",
      "Episode: 300 Total reward: 56.0 Training loss: 2.2544 Explore Prob: 0.4235\n",
      "Model Saved\n",
      "Episode: 301 Total reward: 76.0 Training loss: 4.6363 Explore Prob: 0.4227\n",
      "Episode: 302 Total reward: 76.0 Training loss: 8.2829 Explore Prob: 0.4219\n",
      "Model Saved\n",
      "Episode: 303 Total reward: 92.0 Training loss: 1.9901 Explore Prob: 0.4215\n",
      "Episode: 304 Total reward: 95.0 Training loss: 3.4610 Explore Prob: 0.4213\n",
      "Model Saved\n",
      "Episode: 305 Total reward: 95.0 Training loss: 30.1656 Explore Prob: 0.4210\n",
      "Episode: 306 Total reward: 91.0 Training loss: 3.6449 Explore Prob: 0.4206\n",
      "Model Saved\n",
      "Episode: 307 Total reward: 94.0 Training loss: 2.4614 Explore Prob: 0.4203\n",
      "Episode: 308 Total reward: 58.0 Training loss: 1.5012 Explore Prob: 0.4188\n",
      "Model Saved\n",
      "Episode: 309 Total reward: 51.0 Training loss: 2.8513 Explore Prob: 0.4169\n",
      "Episode: 310 Total reward: 95.0 Training loss: 6.4420 Explore Prob: 0.4167\n",
      "Model Saved\n",
      "Episode: 311 Total reward: 95.0 Training loss: 4.7929 Explore Prob: 0.4164\n",
      "Episode: 312 Total reward: 67.0 Training loss: 2.5229 Explore Prob: 0.4153\n",
      "Model Saved\n",
      "Episode: 313 Total reward: 52.0 Training loss: 2.4093 Explore Prob: 0.4137\n",
      "Episode: 314 Total reward: 95.0 Training loss: 3.7524 Explore Prob: 0.4134\n",
      "Model Saved\n",
      "Episode: 315 Total reward: 93.0 Training loss: 2.3966 Explore Prob: 0.4131\n",
      "Episode: 316 Total reward: 76.0 Training loss: 8.6473 Explore Prob: 0.4123\n",
      "Model Saved\n",
      "Episode: 317 Total reward: 95.0 Training loss: 2.5489 Explore Prob: 0.4121\n",
      "Episode: 318 Total reward: 24.0 Training loss: 4.1156 Explore Prob: 0.4096\n",
      "Model Saved\n",
      "Episode: 319 Total reward: 33.0 Training loss: 3.0588 Explore Prob: 0.4075\n",
      "Episode: 320 Total reward: 93.0 Training loss: 5.1285 Explore Prob: 0.4071\n",
      "Model Saved\n",
      "Episode: 321 Total reward: 92.0 Training loss: 3.2497 Explore Prob: 0.4068\n",
      "Episode: 322 Total reward: 69.0 Training loss: 5.6510 Explore Prob: 0.4057\n",
      "Model Saved\n",
      "Episode: 323 Total reward: 95.0 Training loss: 2.9272 Explore Prob: 0.4055\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 324 Total reward: 65.0 Training loss: 4.1076 Explore Prob: 0.4043\n",
      "Model Saved\n",
      "Episode: 325 Total reward: 62.0 Training loss: 7.0483 Explore Prob: 0.4029\n",
      "Episode: 326 Total reward: 94.0 Training loss: 5.6312 Explore Prob: 0.4026\n",
      "Model Saved\n",
      "Episode: 327 Total reward: 95.0 Training loss: 5.5664 Explore Prob: 0.4024\n",
      "Episode: 328 Total reward: 95.0 Training loss: 3.6795 Explore Prob: 0.4022\n",
      "Model Saved\n",
      "Episode: 329 Total reward: 95.0 Training loss: 6.5039 Explore Prob: 0.4019\n",
      "Episode: 330 Total reward: 95.0 Training loss: 7.9480 Explore Prob: 0.4017\n",
      "Model Saved\n",
      "Episode: 331 Total reward: 57.0 Training loss: 4.6162 Explore Prob: 0.4004\n",
      "Episode: 332 Total reward: 50.0 Training loss: 5.1240 Explore Prob: 0.3988\n",
      "Model Saved\n",
      "Episode: 333 Total reward: 95.0 Training loss: 5.2183 Explore Prob: 0.3985\n",
      "Episode: 334 Total reward: 95.0 Training loss: 4.2883 Explore Prob: 0.3983\n",
      "Model Saved\n",
      "Episode: 335 Total reward: 95.0 Training loss: 4.1718 Explore Prob: 0.3981\n",
      "Episode: 336 Total reward: 74.0 Training loss: 2.5450 Explore Prob: 0.3972\n",
      "Model Saved\n",
      "Episode: 337 Total reward: 95.0 Training loss: 3.8601 Explore Prob: 0.3970\n",
      "Episode: 338 Total reward: 74.0 Training loss: 6.8076 Explore Prob: 0.3961\n",
      "Model Saved\n",
      "Episode: 339 Total reward: 95.0 Training loss: 3.9288 Explore Prob: 0.3959\n",
      "Episode: 340 Total reward: 95.0 Training loss: 8.8537 Explore Prob: 0.3957\n",
      "Model Saved\n",
      "Episode: 341 Total reward: 93.0 Training loss: 3.1450 Explore Prob: 0.3954\n",
      "Episode: 342 Total reward: 95.0 Training loss: 5.0580 Explore Prob: 0.3951\n",
      "Model Saved\n",
      "Episode: 343 Total reward: 95.0 Training loss: 6.1428 Explore Prob: 0.3949\n",
      "Episode: 344 Total reward: 69.0 Training loss: 6.9047 Explore Prob: 0.3939\n",
      "Model Saved\n",
      "Episode: 345 Total reward: 93.0 Training loss: 3.1521 Explore Prob: 0.3936\n",
      "Episode: 346 Total reward: 95.0 Training loss: 4.8051 Explore Prob: 0.3933\n",
      "Model Saved\n",
      "Episode: 347 Total reward: 58.0 Training loss: 2.6001 Explore Prob: 0.3919\n",
      "Episode: 348 Total reward: 74.0 Training loss: 6.2448 Explore Prob: 0.3910\n",
      "Model Saved\n",
      "Episode: 349 Total reward: 95.0 Training loss: 6.5430 Explore Prob: 0.3908\n",
      "Episode: 350 Total reward: 65.0 Training loss: 3.3817 Explore Prob: 0.3896\n",
      "Model Saved\n",
      "Episode: 351 Total reward: 95.0 Training loss: 4.3055 Explore Prob: 0.3894\n",
      "Episode: 352 Total reward: 95.0 Training loss: 6.7194 Explore Prob: 0.3892\n",
      "Model Saved\n",
      "Episode: 353 Total reward: 66.0 Training loss: 2.6281 Explore Prob: 0.3880\n",
      "Episode: 354 Total reward: 95.0 Training loss: 6.3735 Explore Prob: 0.3878\n",
      "Model Saved\n",
      "Episode: 355 Total reward: 95.0 Training loss: 2.8073 Explore Prob: 0.3876\n",
      "Episode: 356 Total reward: 75.0 Training loss: 7.8429 Explore Prob: 0.3868\n",
      "Model Saved\n",
      "Episode: 357 Total reward: 74.0 Training loss: 2.7371 Explore Prob: 0.3860\n",
      "Episode: 358 Total reward: 95.0 Training loss: 2.9467 Explore Prob: 0.3857\n",
      "Model Saved\n",
      "Episode: 359 Total reward: 91.0 Training loss: 2.0484 Explore Prob: 0.3854\n",
      "Episode: 360 Total reward: 61.0 Training loss: 3.1666 Explore Prob: 0.3841\n",
      "Model Saved\n",
      "Episode: 361 Total reward: 94.0 Training loss: 1.4819 Explore Prob: 0.3838\n",
      "Episode: 362 Total reward: 69.0 Training loss: 2.6982 Explore Prob: 0.3828\n",
      "Model Saved\n",
      "Episode: 363 Total reward: 30.0 Training loss: 3.3102 Explore Prob: 0.3807\n",
      "Episode: 364 Total reward: 42.0 Training loss: 3.8930 Explore Prob: 0.3789\n",
      "Model Saved\n",
      "Episode: 365 Total reward: 95.0 Training loss: 2.4564 Explore Prob: 0.3787\n",
      "Episode: 366 Total reward: 70.0 Training loss: 7.2410 Explore Prob: 0.3777\n",
      "Model Saved\n",
      "Episode: 367 Total reward: 40.0 Training loss: 2.3967 Explore Prob: 0.3758\n",
      "Episode: 368 Total reward: 95.0 Training loss: 3.1753 Explore Prob: 0.3756\n",
      "Model Saved\n",
      "Episode: 369 Total reward: 95.0 Training loss: 6.0782 Explore Prob: 0.3754\n",
      "Episode: 370 Total reward: 95.0 Training loss: 2.4902 Explore Prob: 0.3752\n",
      "Model Saved\n",
      "Episode: 371 Total reward: 95.0 Training loss: 4.3045 Explore Prob: 0.3750\n",
      "Episode: 372 Total reward: 95.0 Training loss: 4.8572 Explore Prob: 0.3747\n",
      "Model Saved\n",
      "Episode: 373 Total reward: 95.0 Training loss: 3.1341 Explore Prob: 0.3745\n",
      "Episode: 374 Total reward: 95.0 Training loss: 3.9926 Explore Prob: 0.3743\n",
      "Model Saved\n",
      "Episode: 375 Total reward: 95.0 Training loss: 1.9137 Explore Prob: 0.3741\n",
      "Episode: 376 Total reward: 73.0 Training loss: 3.6505 Explore Prob: 0.3733\n",
      "Model Saved\n",
      "Episode: 377 Total reward: 94.0 Training loss: 2.5846 Explore Prob: 0.3730\n",
      "Episode: 378 Total reward: 94.0 Training loss: 48.7809 Explore Prob: 0.3727\n",
      "Model Saved\n",
      "Episode: 379 Total reward: 95.0 Training loss: 2.3539 Explore Prob: 0.3725\n",
      "Episode: 380 Total reward: 95.0 Training loss: 2.6033 Explore Prob: 0.3723\n",
      "Model Saved\n",
      "Episode: 381 Total reward: 55.0 Training loss: 2.2142 Explore Prob: 0.3710\n",
      "Episode: 382 Total reward: 95.0 Training loss: 5.4911 Explore Prob: 0.3708\n",
      "Model Saved\n",
      "Episode: 383 Total reward: 73.0 Training loss: 4.2794 Explore Prob: 0.3698\n",
      "Episode: 384 Total reward: 95.0 Training loss: 2.4174 Explore Prob: 0.3696\n",
      "Model Saved\n",
      "Episode: 385 Total reward: 95.0 Training loss: 4.3816 Explore Prob: 0.3694\n",
      "Episode: 386 Total reward: 92.0 Training loss: 2.7418 Explore Prob: 0.3690\n",
      "Model Saved\n",
      "Episode: 387 Total reward: 58.0 Training loss: 3.7365 Explore Prob: 0.3677\n",
      "Episode: 388 Total reward: 29.0 Training loss: 9.2813 Explore Prob: 0.3656\n",
      "Model Saved\n",
      "Episode: 389 Total reward: 95.0 Training loss: 7.7809 Explore Prob: 0.3654\n",
      "Episode: 390 Total reward: 93.0 Training loss: 4.4999 Explore Prob: 0.3651\n",
      "Model Saved\n",
      "Episode: 391 Total reward: 95.0 Training loss: 4.5080 Explore Prob: 0.3649\n",
      "Episode: 392 Total reward: 56.0 Training loss: 2.9047 Explore Prob: 0.3635\n",
      "Model Saved\n",
      "Episode: 393 Total reward: 95.0 Training loss: 3.3364 Explore Prob: 0.3633\n",
      "Episode: 394 Total reward: 69.0 Training loss: 2.7732 Explore Prob: 0.3623\n",
      "Model Saved\n",
      "Episode: 395 Total reward: 68.0 Training loss: 1.6535 Explore Prob: 0.3614\n",
      "Episode: 396 Total reward: 75.0 Training loss: 3.0209 Explore Prob: 0.3606\n",
      "Model Saved\n",
      "Episode: 397 Total reward: 70.0 Training loss: 2.2371 Explore Prob: 0.3597\n",
      "Episode: 398 Total reward: 95.0 Training loss: 4.5195 Explore Prob: 0.3595\n",
      "Model Saved\n",
      "Episode: 399 Total reward: 74.0 Training loss: 4.9834 Explore Prob: 0.3587\n",
      "Episode: 400 Total reward: 92.0 Training loss: 2.5640 Explore Prob: 0.3584\n",
      "Model Saved\n",
      "Episode: 401 Total reward: 93.0 Training loss: 3.5706 Explore Prob: 0.3581\n",
      "Episode: 402 Total reward: 32.0 Training loss: 4.4148 Explore Prob: 0.3563\n",
      "Model Saved\n",
      "Episode: 403 Total reward: 26.0 Training loss: 3.6804 Explore Prob: 0.3542\n",
      "Episode: 404 Total reward: 95.0 Training loss: 7.6021 Explore Prob: 0.3540\n",
      "Model Saved\n",
      "Episode: 405 Total reward: 71.0 Training loss: 4.2771 Explore Prob: 0.3531\n",
      "Episode: 406 Total reward: 43.0 Training loss: 2.7084 Explore Prob: 0.3515\n",
      "Model Saved\n",
      "Episode: 407 Total reward: 95.0 Training loss: 2.2676 Explore Prob: 0.3513\n",
      "Episode: 408 Total reward: 94.0 Training loss: 2.6168 Explore Prob: 0.3510\n",
      "Model Saved\n",
      "Episode: 409 Total reward: 95.0 Training loss: 2.9012 Explore Prob: 0.3508\n",
      "Episode: 410 Total reward: 95.0 Training loss: 1.9914 Explore Prob: 0.3506\n",
      "Model Saved\n",
      "Episode: 411 Total reward: 74.0 Training loss: 2.7732 Explore Prob: 0.3499\n",
      "Episode: 412 Total reward: 73.0 Training loss: 2.8601 Explore Prob: 0.3491\n",
      "Model Saved\n",
      "Episode: 413 Total reward: 95.0 Training loss: 2.4826 Explore Prob: 0.3489\n",
      "Episode: 414 Total reward: 75.0 Training loss: 4.0650 Explore Prob: 0.3482\n",
      "Model Saved\n",
      "Episode: 415 Total reward: 95.0 Training loss: 2.9875 Explore Prob: 0.3480\n",
      "Episode: 416 Total reward: 44.0 Training loss: 3.6830 Explore Prob: 0.3464\n",
      "Model Saved\n",
      "Episode: 417 Total reward: 46.0 Training loss: 2.6020 Explore Prob: 0.3449\n",
      "Episode: 418 Total reward: 93.0 Training loss: 5.0933 Explore Prob: 0.3446\n",
      "Model Saved\n",
      "Episode: 419 Total reward: 71.0 Training loss: 4.6449 Explore Prob: 0.3438\n",
      "Episode: 420 Total reward: 95.0 Training loss: 3.3999 Explore Prob: 0.3436\n",
      "Model Saved\n",
      "Episode: 421 Total reward: 94.0 Training loss: 2.9322 Explore Prob: 0.3434\n",
      "Episode: 422 Total reward: 35.0 Training loss: 2.9982 Explore Prob: 0.3415\n",
      "Model Saved\n",
      "Episode: 423 Total reward: 95.0 Training loss: 2.3798 Explore Prob: 0.3413\n",
      "Episode: 424 Total reward: 73.0 Training loss: 3.3824 Explore Prob: 0.3405\n",
      "Model Saved\n",
      "Episode: 425 Total reward: 94.0 Training loss: 4.5348 Explore Prob: 0.3403\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 426 Total reward: 60.0 Training loss: 7.9517 Explore Prob: 0.3391\n",
      "Model Saved\n",
      "Episode: 427 Total reward: 95.0 Training loss: 2.3418 Explore Prob: 0.3389\n",
      "Episode: 428 Total reward: 42.0 Training loss: 3.3627 Explore Prob: 0.3373\n",
      "Model Saved\n",
      "Episode: 429 Total reward: 95.0 Training loss: 2.9949 Explore Prob: 0.3371\n",
      "Episode: 430 Total reward: 31.0 Training loss: 6.9399 Explore Prob: 0.3353\n",
      "Model Saved\n",
      "Episode: 431 Total reward: 72.0 Training loss: 10.6187 Explore Prob: 0.3345\n",
      "Episode: 432 Total reward: 36.0 Training loss: 2.2542 Explore Prob: 0.3328\n",
      "Model Saved\n",
      "Episode: 433 Total reward: 37.0 Training loss: 3.2032 Explore Prob: 0.3312\n",
      "Episode: 434 Total reward: 93.0 Training loss: 3.0722 Explore Prob: 0.3309\n",
      "Model Saved\n",
      "Episode: 435 Total reward: 57.0 Training loss: 3.9352 Explore Prob: 0.3297\n",
      "Episode: 436 Total reward: 75.0 Training loss: 4.2225 Explore Prob: 0.3290\n",
      "Model Saved\n",
      "Episode: 437 Total reward: 95.0 Training loss: 5.6772 Explore Prob: 0.3288\n",
      "Episode: 438 Total reward: 95.0 Training loss: 6.8695 Explore Prob: 0.3286\n",
      "Model Saved\n",
      "Episode: 439 Total reward: 90.0 Training loss: 6.3852 Explore Prob: 0.3283\n",
      "Episode: 440 Total reward: 95.0 Training loss: 9.1894 Explore Prob: 0.3281\n",
      "Model Saved\n",
      "Episode: 441 Total reward: 70.0 Training loss: 2.9507 Explore Prob: 0.3273\n",
      "Episode: 442 Total reward: 95.0 Training loss: 2.7759 Explore Prob: 0.3271\n",
      "Model Saved\n",
      "Episode: 443 Total reward: 52.0 Training loss: 12.3350 Explore Prob: 0.3258\n",
      "Episode: 444 Total reward: 64.0 Training loss: 3.5768 Explore Prob: 0.3248\n",
      "Model Saved\n",
      "Episode: 445 Total reward: 71.0 Training loss: 10.3188 Explore Prob: 0.3240\n",
      "Episode: 446 Total reward: 68.0 Training loss: 2.4127 Explore Prob: 0.3232\n",
      "Model Saved\n",
      "Episode: 447 Total reward: 65.0 Training loss: 7.3924 Explore Prob: 0.3222\n",
      "Episode: 448 Total reward: 68.0 Training loss: 2.2171 Explore Prob: 0.3213\n",
      "Model Saved\n",
      "Episode: 449 Total reward: 95.0 Training loss: 4.5011 Explore Prob: 0.3211\n",
      "Episode: 450 Total reward: 95.0 Training loss: 4.6008 Explore Prob: 0.3209\n",
      "Model Saved\n",
      "Episode: 451 Total reward: 72.0 Training loss: 7.3689 Explore Prob: 0.3202\n",
      "Episode: 452 Total reward: 95.0 Training loss: 5.3323 Explore Prob: 0.3200\n",
      "Model Saved\n",
      "Episode: 453 Total reward: 95.0 Training loss: 5.0329 Explore Prob: 0.3198\n",
      "Episode: 454 Total reward: 95.0 Training loss: 8.2597 Explore Prob: 0.3196\n",
      "Model Saved\n",
      "Episode: 455 Total reward: 95.0 Training loss: 2.7284 Explore Prob: 0.3195\n",
      "Episode: 456 Total reward: 95.0 Training loss: 2.5975 Explore Prob: 0.3193\n",
      "Model Saved\n",
      "Episode: 457 Total reward: 95.0 Training loss: 4.1708 Explore Prob: 0.3191\n",
      "Episode: 458 Total reward: 95.0 Training loss: 3.2719 Explore Prob: 0.3189\n",
      "Model Saved\n",
      "Episode: 459 Total reward: 95.0 Training loss: 2.2327 Explore Prob: 0.3187\n",
      "Episode: 460 Total reward: 94.0 Training loss: 3.0367 Explore Prob: 0.3185\n",
      "Model Saved\n",
      "Episode: 461 Total reward: 95.0 Training loss: 4.9381 Explore Prob: 0.3183\n",
      "Episode: 462 Total reward: 95.0 Training loss: 3.5871 Explore Prob: 0.3181\n",
      "Model Saved\n",
      "Episode: 463 Total reward: 50.0 Training loss: 3.1017 Explore Prob: 0.3169\n",
      "Episode: 464 Total reward: 82.0 Training loss: 3.1099 Explore Prob: 0.3163\n",
      "Model Saved\n",
      "Episode: 465 Total reward: 95.0 Training loss: 2.8346 Explore Prob: 0.3161\n",
      "Episode: 466 Total reward: 51.0 Training loss: 3.5154 Explore Prob: 0.3149\n",
      "Model Saved\n",
      "Episode: 467 Total reward: 85.0 Training loss: 2.7237 Explore Prob: 0.3144\n",
      "Episode: 468 Total reward: 19.0 Training loss: 3.0929 Explore Prob: 0.3124\n",
      "Model Saved\n",
      "Episode: 469 Total reward: 75.0 Training loss: 2.4169 Explore Prob: 0.3117\n",
      "Episode: 470 Total reward: 95.0 Training loss: 4.1514 Explore Prob: 0.3115\n",
      "Model Saved\n",
      "Episode: 471 Total reward: 76.0 Training loss: 2.2187 Explore Prob: 0.3109\n",
      "Episode: 472 Total reward: 95.0 Training loss: 4.2612 Explore Prob: 0.3108\n",
      "Model Saved\n",
      "Episode: 473 Total reward: 94.0 Training loss: 3.0505 Explore Prob: 0.3105\n",
      "Episode: 474 Total reward: 95.0 Training loss: 4.0260 Explore Prob: 0.3104\n",
      "Model Saved\n",
      "Episode: 475 Total reward: 95.0 Training loss: 3.4820 Explore Prob: 0.3102\n",
      "Episode: 476 Total reward: 95.0 Training loss: 7.9784 Explore Prob: 0.3100\n",
      "Model Saved\n",
      "Episode: 477 Total reward: 92.0 Training loss: 8.8618 Explore Prob: 0.3097\n",
      "Episode: 478 Total reward: 95.0 Training loss: 2.9073 Explore Prob: 0.3096\n",
      "Model Saved\n",
      "Episode: 479 Total reward: 95.0 Training loss: 5.1330 Explore Prob: 0.3094\n",
      "Episode: 480 Total reward: 75.0 Training loss: 5.1333 Explore Prob: 0.3087\n",
      "Model Saved\n",
      "Episode: 481 Total reward: 76.0 Training loss: 13.1816 Explore Prob: 0.3080\n",
      "Episode: 482 Total reward: 95.0 Training loss: 4.6788 Explore Prob: 0.3078\n",
      "Model Saved\n",
      "Episode: 483 Total reward: 94.0 Training loss: 4.9966 Explore Prob: 0.3076\n",
      "Episode: 484 Total reward: 75.0 Training loss: 5.3981 Explore Prob: 0.3068\n",
      "Model Saved\n",
      "Episode: 485 Total reward: 95.0 Training loss: 8.0378 Explore Prob: 0.3067\n",
      "Episode: 486 Total reward: 56.0 Training loss: 5.7959 Explore Prob: 0.3056\n",
      "Model Saved\n",
      "Episode: 487 Total reward: 93.0 Training loss: 3.2481 Explore Prob: 0.3054\n",
      "Episode: 488 Total reward: 81.0 Training loss: 2.7860 Explore Prob: 0.3048\n",
      "Model Saved\n",
      "Episode: 489 Total reward: 95.0 Training loss: 3.7621 Explore Prob: 0.3046\n",
      "Episode: 490 Total reward: 76.0 Training loss: 9.4779 Explore Prob: 0.3040\n",
      "Model Saved\n",
      "Episode: 491 Total reward: 44.0 Training loss: 4.5159 Explore Prob: 0.3027\n",
      "Episode: 492 Total reward: 95.0 Training loss: 4.2529 Explore Prob: 0.3025\n",
      "Model Saved\n",
      "Episode: 493 Total reward: 71.0 Training loss: 3.3598 Explore Prob: 0.3018\n",
      "Episode: 494 Total reward: 88.0 Training loss: 6.0895 Explore Prob: 0.3014\n",
      "Model Saved\n",
      "Episode: 495 Total reward: 54.0 Training loss: 51.7823 Explore Prob: 0.3002\n",
      "Episode: 496 Total reward: 42.0 Training loss: 5.9281 Explore Prob: 0.2987\n",
      "Model Saved\n",
      "Episode: 497 Total reward: 18.0 Training loss: 5.0799 Explore Prob: 0.2968\n",
      "Episode: 498 Total reward: 95.0 Training loss: 4.2343 Explore Prob: 0.2966\n",
      "Model Saved\n",
      "Episode: 499 Total reward: 89.0 Training loss: 8.6233 Explore Prob: 0.2963\n",
      "Episode: 500 Total reward: 93.0 Training loss: 5.3120 Explore Prob: 0.2960\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "saver = tf.train.Saver()\n",
    "\n",
    "if training == True:\n",
    "    with tf.Session() as sess:\n",
    "        #Training from scratch: comment the line below and uncomment the line next to it\n",
    "        saver.restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "        \n",
    "#         sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        decay_step = 0\n",
    "        \n",
    "        game.init()\n",
    "        \n",
    "        for episode in range(1, total_episodes+1):\n",
    "            step = 0\n",
    "            \n",
    "            episode_rewards = []\n",
    "            \n",
    "            game.new_episode()\n",
    "            state = game.get_state().screen_buffer\n",
    "            \n",
    "            state, stacked_frames = stack_frames(stacked_frames, state,\n",
    "                                                True)\n",
    "            \n",
    "            while step < max_steps:\n",
    "                step += 1\n",
    "                \n",
    "                decay_step += 1  # decaying exploration for each step in the game\n",
    "                \n",
    "                action, explore_probability = predict_action(epsilon_start,\n",
    "                                                            epsilon_end,\n",
    "                                                            decay_rate,\n",
    "                                                            decay_step,\n",
    "                                                            state,\n",
    "                                                            possible_actions)\n",
    "                \n",
    "                reward = game.make_action(action)\n",
    "                \n",
    "                done = game.is_episode_finished()\n",
    "                episode_rewards.append(reward)\n",
    "                \n",
    "                if done:\n",
    "                    next_state = np.zeros((84, 84), dtype = np.int)\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames,\n",
    "                                                             next_state,\n",
    "                                                             False)\n",
    "                    step = max_steps  # ending the current episode\n",
    "                    \n",
    "                    total_rewards = np.sum(episode_rewards)\n",
    "                    \n",
    "                    print(\"Episode: {}\".format(episode),\n",
    "                         \"Total reward: {}\".format(total_rewards),\n",
    "                         \"Training loss: {:.4f}\".format(loss),\n",
    "                         \"Explore Prob: {:.4f}\".format(explore_probability))\n",
    "                    \n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                else:\n",
    "                    next_state = game.get_state().screen_buffer\n",
    "                    next_state, stacked_frames = stack_frames(stacked_frames,\n",
    "                                                             next_state,\n",
    "                                                             False)\n",
    "                    \n",
    "                    memory.add((state, action, reward, next_state, done))\n",
    "                    \n",
    "                    state = next_state\n",
    "                    \n",
    "                # sample a batch from memory   \n",
    "                batch = memory.sample(batch_size)\n",
    "                states_mb = np.array([each[0] for each in batch], ndmin = 3)\n",
    "                actions_mb = np.array([each[1] for each in batch])\n",
    "                rewards_mb = np.array([each[2] for each in batch])\n",
    "                next_states_mb = np.array([each[3] for each in batch], ndmin = 3)\n",
    "                dones_mb = np.array([each[4] for each in batch])\n",
    "                \n",
    "                target_Qs_batch = []\n",
    "                \n",
    "                Qs_next_state = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: next_states_mb})\n",
    "                \n",
    "                for i in range(batch_size):\n",
    "                    terminal = dones_mb[i]\n",
    "                    \n",
    "                    if terminal:\n",
    "                        target_Qs_batch.append(rewards_mb[i])\n",
    "                        \n",
    "                    else:\n",
    "                        target = rewards_mb[i] + gamma*np.max(Qs_next_state[i])\n",
    "                        target_Qs_batch.append(target)\n",
    "                        \n",
    "                targets_mb = np.array([each for each in target_Qs_batch])\n",
    "                \n",
    "                loss, _ = sess.run([DQNetwork.loss, DQNetwork.optimizer],\n",
    "                                  feed_dict = {DQNetwork.inputs_: states_mb,\n",
    "                                              DQNetwork.actions_: actions_mb,\n",
    "                                              DQNetwork.target_Q: targets_mb})\n",
    "                \n",
    "                summary = sess.run(write_op, feed_dict = {DQNetwork.inputs_: states_mb,\n",
    "                                                         DQNetwork.target_Q: targets_mb,\n",
    "                                                         DQNetwork.actions_: actions_mb})\n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "            if episode%2==0:\n",
    "                save_path = saver.save(sess, './models/model.ckpt')\n",
    "                print(\"Model Saved\")\n",
    "                \n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing - Agent plays the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "Game Score:  57.0\n",
      "Game Score:  95.0\n",
      "Game Score:  95.0\n",
      "Game Score:  95.0\n",
      "Game Score:  95.0\n",
      "Game Score:  76.0\n",
      "Game Score:  62.0\n",
      "Game Score:  88.0\n",
      "Game Score:  95.0\n",
      "Game Score:  93.0\n",
      "Game Score:  95.0\n",
      "Game Score:  93.0\n",
      "Game Score:  95.0\n",
      "Game Score:  88.0\n",
      "Game Score:  92.0\n",
      "Game Score:  93.0\n",
      "Game Score:  94.0\n",
      "Game Score:  93.0\n",
      "Game Score:  58.0\n",
      "Game Score:  95.0\n",
      "Game Score:  88.0\n",
      "Game Score:  93.0\n",
      "Game Score:  94.0\n",
      "Game Score:  86.0\n",
      "Game Score:  95.0\n",
      "Game Score:  95.0\n",
      "Game Score:  95.0\n",
      "Game Score:  95.0\n",
      "Game Score:  95.0\n",
      "Game Score:  86.0\n",
      "Game Score:  92.0\n",
      "Game Score:  93.0\n",
      "Game Score:  95.0\n",
      "Game Score:  61.0\n",
      "Game Score:  95.0\n",
      "Game Score:  59.0\n",
      "Game Score:  95.0\n",
      "Game Score:  93.0\n",
      "Game Score:  95.0\n",
      "Game Score:  78.0\n",
      "Game Score:  95.0\n",
      "Game Score:  69.0\n",
      "Game Score:  95.0\n",
      "Game Score:  95.0\n",
      "Game Score:  80.0\n",
      "Game Score:  78.0\n",
      "Game Score:  74.0\n",
      "Game Score:  95.0\n",
      "Game Score:  78.0\n",
      "Game Score:  92.0\n",
      "Game Score:  54.0\n",
      "Game Score:  95.0\n",
      "Game Score:  95.0\n",
      "Game Score:  95.0\n",
      "Game Score:  77.0\n",
      "Game Score:  74.0\n",
      "Game Score:  69.0\n",
      "Game Score:  95.0\n",
      "Game Score:  69.0\n",
      "Game Score:  68.0\n",
      "Game Score:  89.0\n",
      "Game Score:  95.0\n",
      "Game Score:  95.0\n",
      "Game Score:  95.0\n",
      "Game Score:  53.0\n",
      "Game Score:  58.0\n",
      "Game Score:  95.0\n",
      "Game Score:  95.0\n",
      "Game Score:  95.0\n",
      "Game Score:  54.0\n",
      "Game Score:  95.0\n",
      "Game Score:  77.0\n",
      "Game Score:  55.0\n",
      "Game Score:  93.0\n",
      "Game Score:  94.0\n",
      "Game Score:  93.0\n",
      "Game Score:  95.0\n",
      "Game Score:  93.0\n",
      "Game Score:  95.0\n",
      "Game Score:  93.0\n",
      "Game Score:  95.0\n",
      "Game Score:  93.0\n",
      "Game Score:  95.0\n",
      "Game Score:  54.0\n",
      "Game Score:  95.0\n",
      "Game Score:  82.0\n",
      "Game Score:  93.0\n",
      "Game Score:  95.0\n",
      "Game Score:  95.0\n",
      "Game Score:  93.0\n",
      "Game Score:  69.0\n",
      "Game Score:  95.0\n",
      "Game Score:  95.0\n",
      "Game Score:  78.0\n",
      "Game Score:  86.0\n",
      "Game Score:  95.0\n",
      "Game Score:  86.0\n",
      "Game Score:  52.0\n",
      "Game Score:  75.0\n",
      "Game Score:  58.0\n",
      "Average Score:  85.53\n"
     ]
    }
   ],
   "source": [
    "test_episodes = 100\n",
    "average_score = 0\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    game, possible_actions = create_environment()\n",
    "    totalScore = 0\n",
    "    \n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "    game.init()\n",
    "    \n",
    "    \n",
    "    for i in range(test_episodes):\n",
    "        game.new_episode()\n",
    "        \n",
    "        state = game.get_state().screen_buffer\n",
    "        state, stacked_frames = stack_frames(stacked_frames,\n",
    "                                            state, False)\n",
    "        \n",
    "        while not game.is_episode_finished():\n",
    "            Qs = sess.run(DQNetwork.output, feed_dict = {DQNetwork.inputs_: state.reshape((1, *state.shape))})\n",
    "            \n",
    "            choice = np.argmax(Qs)\n",
    "            action = possible_actions[choice]\n",
    "            \n",
    "            game.make_action(action)\n",
    "            done = game.is_episode_finished()\n",
    "            score = game.get_total_reward()\n",
    "                        \n",
    "            if done:\n",
    "                print(\"Game Score: \", score)\n",
    "                average_score += score\n",
    "                break\n",
    "            \n",
    "            next_state = game.get_state().screen_buffer\n",
    "            next_state, stacked_frames = stack_frames(stacked_frames,\n",
    "                                                     next_state, False)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            score = game.get_total_reward()\n",
    "            \n",
    "    print(\"Average Score: \", average_score/test_episodes)\n",
    "    game.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
